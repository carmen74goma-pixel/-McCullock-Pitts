{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNfLKnS80qUhLbwCX1VcIe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carmen74goma-pixel/-McCullock-Pitts/blob/main/Perceptron_Tarea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZhuR3GcZQtV",
        "outputId": "e8154b9b-b318-4d95-e4d1-8b73cf1f3f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos iniciales: [0.37454012 0.95071431], Sesgo inicial: [0.73199394]\n",
            "\n",
            "Iniciando entrenamiento...\n",
            "Convergencia en la época 9.\n",
            "\n",
            "--- Resultados finales ---\n",
            "Pesos finales: [0.37454012 0.95071431]\n",
            "Sesgo final: -0.0680\n",
            "\n",
            "Comprobación de la Operación OR:\n",
            "Entrada: [0 0], Predicción: 0, Esperado: 0 -> CORRECTO\n",
            "Entrada: [0 1], Predicción: 1, Esperado: 1 -> CORRECTO\n",
            "Entrada: [1 0], Predicción: 1, Esperado: 1 -> CORRECTO\n",
            "Entrada: [1 1], Predicción: 1, Esperado: 1 -> CORRECTO\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Entradas para el perceptrón (OR lógico)\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "# 2. Salidas esperadas (Target)\n",
        "Y = np.array([0, 1, 1, 1])\n",
        "\n",
        "# 3. Parámetros de entrenamiento\n",
        "# Inicializamos los pesos aleatoriamente o con valores pequeños.\n",
        "# Se añade un peso extra para el bias si se usa el enfoque extendido,\n",
        "# pero aquí lo manejaremos de forma separada por claridad.\n",
        "np.random.seed(42) # para reproducibilidad\n",
        "weights = np.random.rand(2)\n",
        "# weights = np.array([0.5, 0.5]) # Valores pequeños de inicio\n",
        "\n",
        "# Tasa de aprendizaje\n",
        "lr = 0.1\n",
        "\n",
        "# Número de iteraciones\n",
        "epochs = 50\n",
        "\n",
        "# Sesgo (Inicialmente se debe inicializar a un valor pequeño o cero, no a 1.0)\n",
        "bias = np.random.rand(1)\n",
        "# bias = 0.1 # Opcional: inicializar a un valor fijo pequeño\n",
        "print(f\"Pesos iniciales: {weights}, Sesgo inicial: {bias}\")\n",
        "\n",
        "# Función de activación (Función escalón o Heaviside)\n",
        "def activation_function(net_input):\n",
        "    return 1 if net_input >= 0 else 0\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "print(\"\\nIniciando entrenamiento...\")\n",
        "for epoch in range(epochs):\n",
        "    error_sum = 0\n",
        "    for i in range(len(X)):\n",
        "        # a) Calcular la entrada neta (dot product + bias)\n",
        "        net_input = np.dot(X[i], weights) + bias\n",
        "\n",
        "        # b) Calcular la salida del perceptrón\n",
        "        prediction = activation_function(net_input)\n",
        "\n",
        "        # c) Calcular el error\n",
        "        error = Y[i] - prediction\n",
        "        error_sum += abs(error)\n",
        "\n",
        "        # d) Actualizar pesos y sesgo (Regla de Aprendizaje del Perceptrón)\n",
        "        weights += lr * error * X[i]\n",
        "        bias += lr * error\n",
        "\n",
        "    # Detener si no hay error en la época (convergencia)\n",
        "    if error_sum == 0:\n",
        "        print(f\"Convergencia en la época {epoch + 1}.\")\n",
        "        break\n",
        "\n",
        "print(\"\\n--- Resultados finales ---\")\n",
        "print(f\"Pesos finales: {weights}\")\n",
        "print(f\"Sesgo final: {bias[0]:.4f}\")\n",
        "\n",
        "# Comprobación final\n",
        "print(\"\\nComprobación de la Operación OR:\")\n",
        "for x_in, y_out in zip(X, Y):\n",
        "    net_input = np.dot(x_in, weights) + bias\n",
        "    prediction = activation_function(net_input)\n",
        "    print(f\"Entrada: {x_in}, Predicción: {prediction}, Esperado: {y_out} -> {'CORRECTO' if prediction == y_out else 'FALLO'}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, lr, epochs, weights, bias):\n",
        "        \"\"\"\n",
        "            Constructor del perceptron:\n",
        "            Guarda las variables\n",
        "            lr -> tasa de aprendizaje\n",
        "            epochs -> numero de epocas\n",
        "            weights -> vector de pesos iniciales\n",
        "            bias -> sesgo inicial\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "            Realiza el entrenamiento del Perceptron.\n",
        "        \"\"\"\n",
        "        # Recorre el dataset la cantidad indicada en epocas\n",
        "        for epoch in range(self.epochs):\n",
        "            for j in range(X.shape[0]):\n",
        "                # Calcula la salida del perceptrón para la entrada actual\n",
        "                y_pred = self.activation_function(np.dot(self.weights, X[j]) + self.bias)\n",
        "                # Calcula el error\n",
        "                loss = Y[j] - y_pred\n",
        "                # Actualiza los pesos y el sesgo\n",
        "                self.weights += self.lr * loss * X[j]\n",
        "                self.bias += self.lr * loss\n",
        "            print(f\"Epoch {epoch}, Optimized Weights are {self.weights}, and bias is {self.bias}\")\n",
        "        # Imprime los valores finales de los parámetros aprendidos\n",
        "        print(f\"Optimized Weights are {self.weights} and bias is {self.bias}\")\n",
        "\n",
        "    def activation_function(self, activation):\n",
        "        \"\"\"\n",
        "            Función de activacion escalon\n",
        "        \"\"\"\n",
        "        return 1 if activation >= 0 else 0\n",
        "\n",
        "    def prediction(self, X):\n",
        "        \"\"\"\n",
        "            Calcula la salida del Perceptron para cada fila de entradas X.\n",
        "        \"\"\"\n",
        "        # Calcula producto punto + bias para todas las entradas\n",
        "        sum_ = np.dot(X, self.weights) + self.bias\n",
        "        # Mensaje input y su predicción\n",
        "        for i, s in enumerate(sum_):\n",
        "            print(f\"Input: {X[i]}, Predictions: {self.activation_function(sum_[i])}\")\n",
        "        # Devuelve un array con todas las predicciones\n",
        "        return np.array([self.activation_function(s) for s in sum_])"
      ],
      "metadata": {
        "id": "b2D8fOHCbAQL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una instancia del perceptrón\n",
        "p = Perceptron(lr=lr, epochs=epochs, weights=weights, bias=bias)\n",
        "# Entrenar el modelo\n",
        "p.fit(X, Y)\n",
        "# Usar el modelo entrenado para realizar predicciones\n",
        "predictions = p.prediction(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7sMu9GpbEpj",
        "outputId": "b4df62c8-1b9a-48e1-8174-6a3c0e902036"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 1, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 2, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 3, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 4, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 5, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 6, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 7, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 8, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 9, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 10, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 11, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 12, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 13, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 14, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 15, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 16, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 17, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 18, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 19, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 20, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 21, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 22, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 23, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 24, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 25, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 26, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 27, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 28, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 29, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 30, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 31, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 32, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 33, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 34, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 35, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 36, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 37, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 38, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 39, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 40, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 41, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 42, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 43, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 44, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 45, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 46, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 47, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 48, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Epoch 49, Optimized Weights are [0.37454012 0.95071431], and bias is [-0.06800606]\n",
            "Optimized Weights are [0.37454012 0.95071431] and bias is [-0.06800606]\n",
            "Input: [0 0], Predictions: 0\n",
            "Input: [0 1], Predictions: 1\n",
            "Input: [1 0], Predictions: 1\n",
            "Input: [1 1], Predictions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Código del Perceptrón para NOT\n",
        "import numpy as np\n",
        "\n",
        "# 1. Entradas para el perceptrón (NOT lógico: solo 1 columna)\n",
        "X = np.array([[0],\n",
        "              [1]])\n",
        "\n",
        "# 2. Salidas esperadas (Target)\n",
        "Y = np.array([1, 0])\n",
        "\n",
        "# 3. Parámetros de entrenamiento\n",
        "# Solo necesitamos 1 peso para la única entrada (X).\n",
        "np.random.seed(43) # Nueva semilla para NOT\n",
        "weights = np.random.rand(1)\n",
        "\n",
        "# Tasa de aprendizaje\n",
        "lr = 0.1\n",
        "\n",
        "# Número de iteraciones\n",
        "epochs = 50\n",
        "\n",
        "# Sesgo (Bias)\n",
        "bias = np.random.rand(1)\n",
        "print(f\"Pesos iniciales: {weights}, Sesgo inicial: {bias}\")\n",
        "\n",
        "# Función de activación (Función escalón)\n",
        "def activation_function(net_input):\n",
        "    return 1 if net_input >= 0 else 0\n",
        "\n",
        "# Bucle de entrenamiento\n",
        "print(\"\\nIniciando entrenamiento...\")\n",
        "for epoch in range(epochs):\n",
        "    error_sum = 0\n",
        "    for i in range(len(X)):\n",
        "        # a) Calcular la entrada neta (dot product + bias)\n",
        "        # X[i] es ahora un array de un solo elemento, p. ej. [0] o [1]\n",
        "        net_input = np.dot(X[i], weights) + bias\n",
        "\n",
        "        # b) Calcular la salida del perceptrón\n",
        "        prediction = activation_function(net_input)\n",
        "\n",
        "        # c) Calcular el error\n",
        "        error = Y[i] - prediction\n",
        "        error_sum += abs(error)\n",
        "\n",
        "        # d) Actualizar pesos y sesgo\n",
        "        weights += lr * error * X[i]\n",
        "        bias += lr * error\n",
        "\n",
        "    # Detener si no hay error\n",
        "    if error_sum == 0:\n",
        "        print(f\"Convergencia en la época {epoch + 1}.\")\n",
        "        break\n",
        "\n",
        "print(\"\\n--- Resultados finales ---\")\n",
        "print(f\"Peso final: {weights[0]:.4f}\")\n",
        "print(f\"Sesgo final: {bias[0]:.4f}\")\n",
        "\n",
        "# Comprobación final\n",
        "print(\"\\nComprobación de la Operación NOT:\")\n",
        "for x_in, y_out in zip(X, Y):\n",
        "    net_input = np.dot(x_in, weights) + bias\n",
        "    prediction = activation_function(net_input)\n",
        "    print(f\"Entrada: {x_in[0]}, Predicción: {prediction}, Esperado: {y_out} -> {'CORRECTO' if prediction == y_out else 'FALLO'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TktPGXSYdMSh",
        "outputId": "427c17aa-33a3-4a90-8ac5-559f9a672bee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pesos iniciales: [0.11505457], Sesgo inicial: [0.60906654]\n",
            "\n",
            "Iniciando entrenamiento...\n",
            "Convergencia en la época 5.\n",
            "\n",
            "--- Resultados finales ---\n",
            "Peso final: -0.2849\n",
            "Sesgo final: 0.2091\n",
            "\n",
            "Comprobación de la Operación NOT:\n",
            "Entrada: 0, Predicción: 1, Esperado: 1 -> CORRECTO\n",
            "Entrada: 1, Predicción: 0, Esperado: 0 -> CORRECTO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, lr, epochs, weights, bias):\n",
        "        \"\"\"\n",
        "            Constructor del perceptron:\n",
        "            Guarda las variables\n",
        "            lr -> tasa de aprendizaje\n",
        "            epochs -> numero de epocas\n",
        "            weights -> vector de pesos iniciales\n",
        "            bias -> sesgo inicial\n",
        "        \"\"\"\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        \"\"\"\n",
        "            Realiza el entrenamiento del Perceptron.\n",
        "        \"\"\"\n",
        "        # Recorre el dataset la cantidad indicada en epocas\n",
        "        for epoch in range(self.epochs):\n",
        "            for j in range(X.shape[0]):\n",
        "                # Calcula la salida del perceptrón para la entrada actual\n",
        "                y_pred = self.activation_function(np.dot(self.weights, X[j]) + self.bias)\n",
        "                # Calcula el error\n",
        "                loss = Y[j] - y_pred\n",
        "                # Actualiza los pesos y el sesgo\n",
        "                self.weights += self.lr * loss * X[j]\n",
        "                self.bias += self.lr * loss\n",
        "            print(f\"Epoch {epoch}, Optimized Weights are {self.weights}, and bias is {self.bias}\")\n",
        "        # Imprime los valores finales de los parámetros aprendidos\n",
        "        print(f\"Optimized Weights are {self.weights} and bias is {self.bias}\")\n",
        "\n",
        "    def activation_function(self, activation):\n",
        "        \"\"\"\n",
        "            Función de activacion escalon\n",
        "        \"\"\"\n",
        "        return 1 if activation >= 0 else 0\n",
        "\n",
        "    def prediction(self, X):\n",
        "        \"\"\"\n",
        "            Calcula la salida del Perceptron para cada fila de entradas X.\n",
        "        \"\"\"\n",
        "        # Calcula producto punto + bias para todas las entradas\n",
        "        sum_ = np.dot(X, self.weights) + self.bias\n",
        "        # Mensaje input y su predicción\n",
        "        for i, s in enumerate(sum_):\n",
        "            print(f\"Input: {X[i]}, Predictions: {self.activation_function(sum_[i])}\")\n",
        "        # Devuelve un array con todas las predicciones\n",
        "        return np.array([self.activation_function(s) for s in sum_])"
      ],
      "metadata": {
        "id": "NJc3kNrgdnub"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una instancia del perceptrón\n",
        "p = Perceptron(lr=lr, epochs=epochs, weights=weights, bias=bias)\n",
        "# Entrenar el modelo\n",
        "p.fit(X, Y)\n",
        "# Usar el modelo entrenado para realizar predicciones\n",
        "predictions = p.prediction(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nk9NYWTd4pD",
        "outputId": "365df33e-04b8-4eec-a52c-c643b27d9417"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 1, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 2, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 3, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 4, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 5, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 6, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 7, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 8, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 9, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 10, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 11, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 12, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 13, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 14, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 15, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 16, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 17, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 18, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 19, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 20, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 21, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 22, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 23, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 24, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 25, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 26, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 27, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 28, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 29, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 30, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 31, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 32, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 33, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 34, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 35, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 36, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 37, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 38, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 39, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 40, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 41, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 42, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 43, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 44, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 45, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 46, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 47, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 48, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Epoch 49, Optimized Weights are [-0.28494543], and bias is [0.20906654]\n",
            "Optimized Weights are [-0.28494543] and bias is [0.20906654]\n",
            "Input: [0], Predictions: 1\n",
            "Input: [1], Predictions: 0\n"
          ]
        }
      ]
    }
  ]
}